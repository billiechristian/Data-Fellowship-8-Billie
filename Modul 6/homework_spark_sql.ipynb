{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .parquet('fhv_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Billie\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   35709|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many taxi trips were there on February 15?\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    select count(*) \n",
    "    from fhv\n",
    "    where to_date(pickup_datetime) = '2021-02-15'\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date|max_duration_minutes|\n",
      "+----------+--------------------+\n",
      "|2021-02-05|            110919.0|\n",
      "|2021-02-01|             46290.0|\n",
      "|2021-02-25|             40489.0|\n",
      "|2021-02-23|             40352.0|\n",
      "|2021-02-04|   40034.88333333333|\n",
      "|2021-02-27|             17084.0|\n",
      "|2021-02-28|             15763.0|\n",
      "|2021-02-15|            14670.15|\n",
      "|2021-02-22|  13001.533333333333|\n",
      "|2021-02-08|   9424.916666666666|\n",
      "|2021-02-19|             9012.15|\n",
      "|2021-02-13|   8422.683333333332|\n",
      "|2021-02-16|              4816.1|\n",
      "|2021-02-12|              4344.0|\n",
      "|2021-02-17|   4284.783333333334|\n",
      "|2021-02-11|  3219.8166666666666|\n",
      "|2021-02-24|   2767.733333333333|\n",
      "|2021-02-06|   2752.633333333333|\n",
      "|2021-02-18|  2749.0333333333333|\n",
      "|2021-02-20|  2701.4666666666667|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the longest trip for each day ?\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    with duration_cte as (\n",
    "        select\n",
    "            to_date(pickup_datetime) as date,\n",
    "            ((bigint(to_timestamp(dropOff_datetime))) - (bigint(to_timestamp(pickup_datetime))))/60 as durations\n",
    "        from fhv\n",
    "    )\n",
    "    select date, max(durations) as max_duration_minutes \n",
    "    from duration_cte\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|dispatching_base_num|count|\n",
      "+--------------------+-----+\n",
      "|              B00856|35077|\n",
      "|              B01312|33089|\n",
      "|              B01145|31114|\n",
      "|              B02794|30397|\n",
      "|              B03016|29794|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find Top 5 Most frequent `dispatching_base_num` ?\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    select dispatching_base_num\n",
    "    , count(*) as count\n",
    "    from fhv\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    limit 5\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|PUlocationID|DOlocationID|count|\n",
      "+------------+------------+-----+\n",
      "|       206.0|       206.0| 2374|\n",
      "|       221.0|       206.0| 2112|\n",
      "|       129.0|       129.0| 1902|\n",
      "|         7.0|         7.0| 1829|\n",
      "|       179.0|       179.0| 1736|\n",
      "+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find Top 5 Most common location pairs (PUlocationID and DOlocationID)\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    select \n",
    "        PUlocationID\n",
    "        , DOlocationID\n",
    "        , count(*) as count\n",
    "    from fhv\n",
    "    where PUlocationID is not null and DOlocationID is not null\n",
    "    group by 1,2\n",
    "    order by 3 desc\n",
    "    limit 5\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing to Bigquery and GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = r'C:\\Users\\Billie\\.google\\credentials\\google_credentials.json'\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .parquet('fhv_tripdata_2021-02.parquet')\n",
    "df.createOrReplaceTempView('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many taxi trips were there on February 15?\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    select count(distinct dispatching_base_num) from fhv\n",
    "    \"\"\"\n",
    "\n",
    "task_1 = spark.sql(query)\n",
    "task_1.show()\n",
    "task_1.printSchema()\n",
    "\n",
    "task_1.write.format('bigquery') \\\n",
    "  .option('table', 'homework_modul6.task_1') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the longest trip for each day ?\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    with duration_cte as (\n",
    "        select\n",
    "            to_date(pickup_datetime) as date,\n",
    "            ((bigint(to_timestamp(dropOff_datetime))) - (bigint(to_timestamp(pickup_datetime))))/60 as durations\n",
    "        from fhv\n",
    "    )\n",
    "    select date, max(durations) as max_duration_minutes \n",
    "    from duration_cte\n",
    "    group by 1\n",
    "    order by 1\n",
    "    \"\"\"\n",
    "\n",
    "task_2 = spark.sql(query)\n",
    "task_2.show()\n",
    "task_2.printSchema()\n",
    "\n",
    "task_2.write.format('bigquery') \\\n",
    "  .option('table', 'homework_modul6.task_2') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Top 5 Most frequent `dispatching_base_num` ?\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    select dispatching_base_num\n",
    "    , count(*) as count\n",
    "    from fhv\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    limit 5\n",
    "    \"\"\"\n",
    "\n",
    "task_3 = spark.sql(query)\n",
    "task_3.show()\n",
    "task_3.printSchema()\n",
    "\n",
    "task_3.write.format('bigquery') \\\n",
    "  .option('table', 'homework_modul6.task_3') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Top 5 Most common location pairs (PUlocationID and DOlocationID)\n",
    "query = \\\n",
    "    \"\"\"\n",
    "    select \n",
    "        PUlocationID\n",
    "        , DOlocationID\n",
    "        , count(*) as count\n",
    "    from fhv\n",
    "    where PUlocationID is not null and DOlocationID is not null\n",
    "    group by 1,2\n",
    "    order by 3 desc\n",
    "    limit 5\n",
    "    \"\"\"\n",
    "\n",
    "task_4 = spark.sql(query)\n",
    "task_4.show()\n",
    "task_4.printSchema()\n",
    "\n",
    "task_4.write.format('bigquery') \\\n",
    "  .option('table', 'homework_modul6.task_4') \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1624ab4806a020fb463aa4701ef9918ab310fce9a4f90ba27babafda5f9ac8f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
